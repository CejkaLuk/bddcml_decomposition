\input texinfo  @c                                  -*- Texinfo -*-
@setfilename bddcml.info

@ifinfo
@format
START-INFO-DIR-ENTRY
* BDDCML: (bddcml).		A Multi-Level Balancing Domain Decomposition by Constraints solver library.
END-INFO-DIR-ENTRY
@end format

@end ifinfo

@iftex
@c @finalout
@settitle BDDCML
@titlepage
@title BDDCML
@subtitle solver library based on Multi-Level Balancing Domain Decomposition by Constraints
@subtitle copyright (C) 2010-2011 Jakub @v{S}@'{@dotless{i}}stek
@subtitle version 1.1
@author Jakub @v{S}@'{@dotless{i}}stek

@end titlepage

@parindent 0mm
@parskip 2mm

@end iftex
@node Top,,, (dir)

@c table of contents
@contents

@chapter Introduction

The BDDCML (Balancing Domain Decomposition by Constraints - Multi-Level) is a library for solving large sparse linear systems resulting from 
computations by the finite element method (FEM). 
Domain decomposition technique is employed which allows distribution of the computations among processors.

The main goal of the package is to provide a scalable implementation of the (Adaptive) Multilevel BDDC method. 
Codes are written in Fortran 95 with MPI library. 
A library is provided, which is supposed to be called from users' applications. 
It provides a simple interface functions callable from Fortran and C. 

Balancing Domain Decomposition by Constraints (BDDC) has quickly evolved into a very popular method. However, for very large numbers of subdomain, the coarse problem becomes a large problem to be solved in its own right. In Multilevel BDDC, the coarse problem is solved only approximately by recursive application of BDDC to higher levels.

The main web site of the BDDCML project is

@uref{http://www.math.cas.cz/~sistek/software/bddcml.html}

In case of questions, reporting a bug, or just of interest, feel free to contact 
Jakub @v{S}@'{@dotless{i}}stek 
at
@email{sistek@@math.cas.cz}.


@chapter How to use @code{BDDCML}

The library provides a simple interface callable from Fortran and C. 
Although main parts of the solver are purely algebraic, the solver needs also to get some information of the computational mesh.
This requirement is mainly motivated by selection of corners within the method, for which existing algorithms rely on geometry.

Two different modes are possible for input:
@itemize
@item user can either provide information about global mesh and a file with element matrices (global loading),
@item user can provide division into subdomains on the first level and pass 
  subdomain matrices for each subdomain to the routine (local loading).
@end itemize

The solution process is divided into the following sequence of called functions.
Their parameters are described in separate sections.

@enumerate
@item @code{bddcml_init} -- initialization of the solver
@item
@itemize
@item @code{bddcml_upload_global_data} -- loading global data about computational mesh and matrix (use for global loading)
@item @code{bddcml_upload_subdomain_data} -- loading data for one subdomain mesh and matrix (use for local loading)
@end itemize
@item @code{bddcml_setup_preconditioner} -- prepare preconditioner 
@item @code{bddcml_solve} -- solve loaded system by precodnitioned Krylov subspace iterative method (PCG or BiCGstab)
@item 
@itemize
@item @code{bddcml_download_local_solution} -- get the solution restricted to a subdomain from the solver (use for local loading)
@item @code{bddcml_download_global_solution} -- get the global solution from the solver (use for global loading)
@end itemize
@item @code{bddcml_finalize} -- clear solver data and deallocate memory
@end enumerate

Two examples are presented in the @code{examples} folder. Both are written in Fortran 90. 
The @file{bddcml_global.f90} demonstrates the use of global input, while the @file{bddcml_local.f90} demonstrates the use of localized subdomain input.


@chapter Description of interface functions

In this chapter, detailed description of the solver interface functions with explanation of individual arguments is given

@section @code{bddcml_init}

@unnumberedsubsec C interface
@code{
void bddcml_init( int *nl, int *nsublev, int *lnsublev, int *nsub_loc_1, int *comm_init, int *verbose_level, int *numbase )
}

@unnumberedsubsec Description
Prepares internal data structures for the solver. 

@unnumberedsubsec Parameters

@table @code
@item nl
given number of levels

@item nsublev
array with GLOBAL numbers of subdomains for each level

@item lnsublev
length of array @code{nsublev} - should match @code{nl}

@item nsub_loc_1 
LOCAL number of subdomains assigned to the process.
@itemize
@item >= 0 number of local subdomains - sum up across processes to nsublev[0]
@item -1   let solver decide, the value is returned ( determining linear partition )
@end itemize

@item comm_init 
initial global communicator (possibly @code{MPI_COMM_WORLD}). This should be communicator in Fortran.
When called from C, it should NOT be of type @code{MPI_Comm}. 
Use @code{MPI_Comm_c2f} function before calling this routine to get the proper argument. 

@item verbose_level 
level of verbosity
@itemize
@item     0 - only errors printed
@item     1 - some output
@item     2 - detailed output
@end itemize

@item numbase
first index of arrays ( 0 for C, 1 for Fortran )
@end table 



@page
@section @code{bddcml_upload_global_data}

@unnumberedsubsec C interface
@code{
void bddcml_upload_global_data( int *nelem, int *nnod, int *ndof, int *ndim, int *meshdim, 
                                int *inet, int *linet, int *nnet, int *lnnet, int *nndf, int *lnndf, 
                                double *xyz, int *lxyz1, int *lxyz2,
                                int *ifix, int *lifix, double *fixv, int *lfixv, double *rhs, int *lrhs, double *sol, int *lsol, int *idelm, 
                                int *neighbouring, int *load_division_int )
}

@unnumberedsubsec Description
If no distribution of data exists in the user application, it may be left to the solver. 
This routine loads global information on mesh connectivity and coordinates. 
Matrix is passed as unassembled matrices of individual elements which will be read from opened file unit @code{idelm} 
and assembled within the solver.
If partitionining into subdomains on the basic level exists in user's application, 
routine @code{bddcml_upload_subdomain_data} should be used instead.


@unnumberedsubsec Parameters

@table @code

@item nelem
GLOBAL number of elements

@item nnod
GLOBAL number of nodes

@item ndof
GLOBAL number of degrees of freedom, i.e. size of matrix

@item ndim 
number of space dimensions

@item meshdim
mesh dimension.  For 3D elements = @code{ndim}, for 3D shells = 2, for 3D beams  = 1

@item inet
GLOBAL array with Indices of Nodes on ElemenTs - this defines connectivity of the mesh. 

@item linet
length of array @code{inet}. It is given as a sum of entries in array @code{nnet}.

@item nnet
GLOBAL array with Number of Nodes on ElemenTs. For each element, it gives number of nodes it is connected to.
This is important to locate element entries in array @code{inet}

@item lnnet
length of array @code{nnet}. It is equal to @code{nelem}.

@item nndf
GLOBAL array with Number of Nodal Degrees of Freedom. For each node, it gives number of attached degrees of freedom.

@item lnndf
length of array @code{nndf}. It is equal to @code{nnod}.

@item xyz
GLOBAL Coordinates of nodes as one array (all X, all Y, all Z) or as two-dimensional array in Fortran (X | Y | Z).
Rows are defined by nodes, columns are defined by dimension.

@item lxyz1,lxyz2
dimensions of array @code{xyz}. In C, lenght of @code{xyz} is defined as @code{lxyz1 * lxyz2}. In Fortran,
dimension of @code{xyz} is given used as @code{xyz(lxyz1,lxyz2)}.
The @code{lxyz1} is equal to @code{nnod}. The @code{lxyz2} is equal to @code{ndim}.

@item ifix
GLOBAL array of Indices of FIXed variables - all degrees of freedom with Dirichlet BC are marked with its number, 
i.e. non-zero entries determine fixed degrees of freedom.

@item lifix
length of array @code{ifix}, equal to @code{ndof}.

@item fixv
GLOBAL array of FIXed Variables - where @code{ifix} is non-zero, @code{fixv} stores value of Dirichlet boundary condition. Where @code{ifix} is zero, corresponding value in @code{fixv} is meaningless.

@item lfixv
length of array @code{fixv}, equal to @code{ndof}.

@item rhs
GLOBAL array with Right-Hand Side

@item lrhs
length of array @code{rhs}, equal to @code{ndof}.

@item sol
GLOBAL array with initial SOLution guess. This is used as initial approximation for iterative method.

@item lsol
length of array @code{sol}, equal to @code{ndof}.

@item idelm
opened Fortran unit with unformatted file with element matrices

@item neighbouring 
how many nodes should be shared by two elements to call them adjacent in graph. 
This parameter is used for division of mesh on the basic level by @code{ParMETIS} or @code{METIS}. 
Often, one gets better results if he specifies this number to define adjacency only if elements share a face in 3D
or edge in 2D. E.g. for linear tetrahedra, the recommended value is 3.

@item load_division_int
Should division from file @file{partition_l1.ES} be used? ( 0 - partition is created in the solver, 1 - partition is read)
If partition is read, the file contains for each element, number of subdomain it belongs to.
Begins from 1.

@end table


@page
@section @code{bddcml_upload_subdomain_data}

@unnumberedsubsec C interface
@code{
void bddcml_upload_subdomain_data( int *nelem, int *nnod, int *ndof, int *ndim, int *meshdim,
                                   int *isub, int *nelems, int *nnods, int *ndofs, 
                                   int *inet, int *linet, int *nnet, int *lnnet, int *nndf, int *lnndf, 
                                   int *isngn, int *lisngn, int *isvgvn, int *lisvgvn, int *isegn, int *lisegn, 
                                   double *xyz, int *lxyz1, int *lxyz2, 
                                   int *ifix, int *lifix, double *fixv, int *lfixv, 
                                   double *rhs, int *lrhs, 
                                   double *sol, int *lsol, 
                                   int *matrixtype, int *i_sparse, int *j_sparse, double *a_sparse, int *la, int *is_assembled_int )
}

@unnumberedsubsec Description
If distribution of data into subdomains exists already in the user application, data should be loaded into the solver using this routine.
It may be called repeatedly by each process if more than one subdomain are assigned to that process.
It loads the local mesh of the subdomain and assembled subdomain matrix in the coordinate format. 
Most data are localized to subdomain.

If partitionining into subdomains does not exist in user's application, 
routine @code{bddcml_upload_global_data} should be preferred.


@unnumberedsubsec Parameters

@table @code

@item nelem
GLOBAL number of elements

@item nnod
GLOBAL number of nodes

@item ndof
GLOBAL number of degrees of freedom, i.e. size of matrix

@item ndim 
number of space dimensions

@item meshdim
mesh dimension.  For 3D elements = @code{ndim}, for 3D shells = 2, for 3D beams  = 1

@item isub
GLOBAL index of subdomain which is loaded

@item nelems
LOCAL number of elements in subdomain

@item nnods
LOCAL number of nodes in subdomain mesh

@item ndofs
LOCAL number of degrees of freedom in subdomain mesh

@item inet
LOCAL array with Indices of Nodes on ElemenTs - this defines connectivity of the subdomain mesh. 

@item linet
length of array @code{inet}. It is given as a sum of entries in array @code{nnet}.

@item nnet
LOCAL array with Number of Nodes on ElemenTs. For each element, it gives number of nodes it is connected to.
This is important to locate element entries in array @code{inet}

@item lnnet
length of array @code{nnet}. It is equal to @code{nelems}.

@item nndf
LOCAL array with Number of Nodal Degrees of Freedom. For each node, it gives number of attached degrees of freedom.

@item lnndf
length of array @code{nndf}. It is equal to @code{nnods}.

@item isngn
array of Indices of Subdomain Nodes in Global Numbering (local to global map of nodes). For each local node gives the global index in original mesh.

@item lisngn
length of array @code{isngn}. It is equal to @code{nnods}.

@item isvgvn
array of Indices of Subdomain Variables in Global Variable Numbering (local to global map of variables). For each local degree of freedom gives the global index in original matrix.

@item lisvgvn
length of array @code{isvgvn}. It is equal to @code{ndofs}.

@item isegn
array of Indices of Subdomain Elements in Global Numbering (local to global map of elements). For each subdomain element gives global number in original mesh.

@item lisegn
length of array @code{isegn}. It is equal to @code{nelems}.

@item xyz
LOCAL array with coordinates of nodes as one array (all X, all Y, all Z) or as two-dimensional array in Fortran (X | Y | Z).
Rows are defined by nodes, columns are defined by dimension.

@item lxyz1,lxyz2
dimensions of array @code{xyz}. In C, lenght of @code{xyz} is defined as @code{lxyz1 * lxyz2}. In Fortran,
dimension of @code{xyz} is used as @code{xyz(lxyz1,lxyz2)}. 
The @code{lxyz1} is equal to @code{nnods}. The @code{lxyz2} is equal to @code{ndim}.

@item ifix
LOCAL array of Indices of FIXed variables - all dofs with Dirichlet boundary condition are marked with its number, 
i.e. non-zero entries determine fixed degrees of freedom.

@item lifix
length of array @code{ifix}, equal to @code{ndofs}.

@item fixv
LOCAL array of FIXed Variables - where @code{ifix} is non-zero, @code{fixv} stores value of Dirichlet boundary condition. Where @code{ifix} is zero, corresponding value in @code{fixv} is meaningless.

@item lfixv
length of array @code{fixv}, equal to @code{ndofs}.

@item rhs
LOCAL array with Right-Hand Side. Values at nodes repeated among subdomains are copied and not weighted. 

@item lrhs
length of array @code{rhs}, equal to @code{ndofs}.

@item sol
LOCAL array with initial SOLution guess. This is used as initial approximation for iterative method. 

@item lsol
length of array @code{sol}, equal to @code{ndofs}.

@item matrixtype 
Type of the matrix. This parameter determines storage and underlying direct method of the @code{MUMPS} solver for factorizations.
Matrix is loaded in coordinate format by three arrays described below. Options are
@itemize
@item 0 
unsymmetric - whole matrix is loaded
@item 1 
symmetric positive definite - only upper triangle of the matrix is loaded
@item 2 
general symmetric - only upper triangle of the matrix is loaded
@end itemize

@item i_sparse
array of row indices of non-zero entries

@item j_sparse
array of column indices of non-zero entries

@item a_sparse  
array of values of non-zero entries

@item la  
length of previous arrays @code{i_sparse}, @code{j_sparse}, @code{a_sparse} ( equal to number of non-zeros if the matrix is loaded already assembled )

@item is_assembled_int  
is the matrix assembled? 
The solver comes with fast assembly routine so the users might want to pass just unassembled matrix for each subdomain 
(i.e. copy of element matrices equipped with global indexing), and let the solver assemble it.
@itemize 
@item 0 - no, it can contain repeated entries, will be assembled by solver
@item 1 - yes, it is sorted and does not contain repeated index pairs
@end itemize 

@end table


@page
@section @code{bddcml_setup_preconditioner}

@unnumberedsubsec C interface
@code{
void bddcml_setup_preconditioner( int *matrixtype, int *use_defaults_int,
                                  int *parallel_division_int, int *use_arithmetic_int, int *use_adaptive_int );
}

@unnumberedsubsec Description
Calling this function prepares internal data of the preconditioner. Local factorizations are performed for each subdomain at each level and also
the resulting coarse problem on the final level is factored. This might be quite costly routine. 
Once the preconditioner is set-up, it can be reused for new right hand sides (if the matrix is not changed) 
by calling @code{bddcml_upload_subdomain_data} followed by @code{bddcml_solve}.

@unnumberedsubsec Parameters

@table @code

@item matrixtype 
Type of the matrix. This parameter determines storage and underlying direct method of the @code{MUMPS} solver for factorizations.
Should keep the value inserted to @code{bddcml_upload_subdomain_data}. Options are
@itemize
@item 0 
unsymmetric - whole matrix is loaded
@item 1 
symmetric positive definite - only upper triangle of the matrix is loaded
@item 2 
general symmetric - only upper triangle of the matrix is loaded
@end itemize

@item use_defaults_int
If @code{> 0}, other options are ignored and the solver uses default options.

@item parallel_division_int
If @code{> 0}, solver will use @code{ParMETIS} to create division on first level. 
This option is only used for global input (@code{bddcml_upload_global_data}) and only applies to the first level. Otherwise, @code{METIS} is used.
Default is @code{1}.

@item use_arithmetic_int
If @code{> 0}, solver will use continuity of arithmetic averages on faces in 2D and faces and edges in 3D to form the coarse space. 
Default is @code{1}.

@item use_adaptive_int
If @code{> 0}, solver will use adaptive averages on faces in 2D and faces in 3D. 
This might be costly and should be used for very ill-conditioned problems. A generalized eigenvalue problem is solved at each face and weighted averages are
derived from eigenvectors. For solving individual eigenproblems, @code{BLOPEX} package is used.
Default is @code{0}.

@end table


@page
@section @code{bddcml_solve}

@unnumberedsubsec C interface
@code{
void bddcml_solve( int *comm_all, int *method, double *tol, int *maxit, int *ndecrmax, 
                   int *num_iter, int *converged_reason, double *condition_number);
}

@unnumberedsubsec Description
This function launches the solution procedure for prepared data. System is solved either by preconditioned conjugate gradient (PCG) method or by 
preconditioned stabilized Bi-Conjugate Gradient (BiCGstab) method.

@unnumberedsubsec Parameters

@table @code
@item comm_all 
global communicator. Should be the same as @code{comm_init} for @code{bddcml_init} function.

@item method
Krylov subspace iterative method
@itemize
@item -1 - use defaults - @code{tol}, @code{maxit}, and @code{ndecrmax} not accessed, BiCGstab method used by default,
@item  0 - use PCG,
@item  1 - use BiCGstab.
@end itemize

@item tol
desired accuracy of relative residual (default 1.e-6).

@item maxit
limit on number of iterations (default 1000).

@item ndecrmax
limit on number of iterations with non-decreasing residual (default 30) - used to stop a diverging process.

@item num_iter
on output, resulting number of iterations.

@item converged_reason
on output, contains reason for convergence/divergence
@itemize
   @item 0 - converged relative residual,
   @item -1 - reached limit on number of iterations,
   @item -2 - reached limit on number of iterations with non-decreasing residual.
@end itemize

@item condition_number
on output, estimated condition number ( for PCG only ).

@end table


@page
@section @code{bddcml_download_local_solution}

@unnumberedsubsec C interface
@code{
void bddcml_download_local_solution( int *isub, double *sols, int *lsols )
}

@unnumberedsubsec Description
Subroutine for getting local solution, i.e. restriction of solution vector to subdomain (no weights are applied).

@unnumberedsubsec Parameters

@table @code
@item isub
GLOBAL index of subdomain

@item sols
LOCAL array of solution restricted to subdomain

@item lsols
length of array @code{sols}, equal to @code{ndofs}.

@end table


@page
@section @code{bddcml_download_global_solution}

@unnumberedsubsec C interface
@code{
void bddcml_download_global_solution( double *sol, int *lsol )
}

@unnumberedsubsec Description
This function downloads global solution of the system from the solver at root process.

@unnumberedsubsec Parameters

@table @code
@item sol
GLOBAL array of solution

@item lsol
length of array @code{sol}, equal to @code{ndof}

@end table


@page
@section @code{bddcml_dotprod_subdomain}

@unnumberedsubsec C interface
@code{
void bddcml_dotprod_subdomain( int *isub, double *vec1, int *lvec1, double *vec2, int *lvec2, double *dotprod )
}

@unnumberedsubsec Description
Auxiliary subroutine to compute scalar product of two vectors of lenght of
subdomain exploiting interface weights from the solver. This routine is useful 
if we want to compute global norm or dot-product based on vectors restricted to 
subdomains. Since interface values are contained in several vectors for
several subdomains, this dot product or norm cannot be determined without
weights.

@unnumberedsubsec Parameters

@table @code
@item isub
GLOBAL index of subdomain

@item vec1
LOCAL first vector for dot-product

@item lvec1        
length of @code{vec1}

@item vec2
LOCAL second vector for dot-product, may be same array as @code{vec1}

@item lvec2        
length of @code{vec2}, should be same as @code{lvec1}

@item dotprod
on exit, returns vec1' * weights * vec2

@end table


@page
@section @code{bddcml_finalize}

@unnumberedsubsec C interface
@code{
void bddcml_finalize( )
}

@unnumberedsubsec Description
Finalization of the solver. All internal data are deallocated.

@unnumberedsubsec Parameters
This routine currently does not take any arguments.


@bye

